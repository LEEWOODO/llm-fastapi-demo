from dotenv import load_dotenv
from langchain_community.vectorstores import OpenSearchVectorSearch
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from transformers import pipeline
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline

load_dotenv()

# 1. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# 2. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 3. ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
db = OpenSearchVectorSearch(
    index_name="rag-index2",
    embedding_function=embedding,
    opensearch_url="http://localhost:9200",
    http_auth=("admin", "admin")
)

print("ğŸŸ¡ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ!")

# 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
docs = db.similarity_search(query, k=3)
context = "\n".join([doc.page_content for doc in docs])
print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:", len(docs))
print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:", context)


# 5. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="{context} ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ '{question}'ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜."
)

# 6. LLM ì„¤ì • (ì§€ì›ë˜ëŠ” ëª¨ë¸ ì‚¬ìš©)
qa_pipeline = pipeline(
    "text-generation",
    model="tiiuae/falcon-rw-1b",  # âœ… CPUì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
    device=-1
)

llm = HuggingFacePipeline(
    pipeline=qa_pipeline,
    model_kwargs={"max_new_tokens": 100, "temperature": 0.7}
)

# 7. íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰
chain = RunnableLambda(lambda question: FalconLLMProvider.invoke(
    prompt_template.format(context=context, question=question)
))

response = chain.invoke(query)
print("\nğŸ¤– ë‹µë³€:", response.content if hasattr(response, "content") else response)