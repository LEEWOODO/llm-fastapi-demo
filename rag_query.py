from dotenv import load_dotenv
from langchain_community.vectorstores import OpenSearchVectorSearch
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

load_dotenv()

# 1. ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# 2. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 3. ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
db = OpenSearchVectorSearch(
    index_name="rag-index2",
    embedding_function=embedding,
    opensearch_url="http://localhost:9200",
    http_auth=("admin", "admin")
)

print("ğŸŸ¡ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ!")

# 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
docs = db.similarity_search(query, k=3)
context = "\n".join([doc.page_content for doc in docs])
print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:", len(docs))
print("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:", context)


# 5. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:
    ---
    {context}
    ---
    ì§ˆë¬¸: {question}
    ë‹µë³€:
    """
)

# 6. LLM ì„¤ì • (Groq ë˜ëŠ” OpenAI ì¤‘ ì„ íƒ)
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0
)

# 7. íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰
chain = RunnableLambda(lambda question: llm.invoke(
    prompt_template.format(context=context, question=question)
))

response = chain.invoke(query)
print("\nğŸ¤– ë‹µë³€:", response.content)